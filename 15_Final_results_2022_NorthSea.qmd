---
title: "15_Final_results_2022_NorthSea"
format: html
---

* Area: North Sea  
* Data until 2022 (NIVA) and 2019-2022 (HI)      

* NIVA levels and trends from Milkys  
* HI levels and trends from data + linear trends    


* Version ver3, manual changes added afterwards:  
    - EQS for HG in blue mussel added  
    - EQS for PCB7 added (see mail from Merete 8.5.2023)  
    - Species and tissue for blue mussel added  

## Packages + functions  
```{r}

library(dplyr)
library(tidyr)
library(readxl)
library(ggplot2)
library(stringr)
library(knitr)    # for kable

# for JAGS (interval regression)  
library(R2jags)
library(rjags)
library(purrr)
source("interval_lm_qi.R")  # function 'int_linear_qi'  

```

## Set up

```{r}

selected_year <- 2022

```

## Lookup tables  

```{r}

# Lookup file for station names  
lookup_stations <- read.csv("../Milkys2_pc/Files_from_Jupyterhub_2022/Lookup_tables/Lookup_stationorder.csv") %>%
  mutate(Station = paste(STATION_CODE, Station_name)) %>%
  select(STATION_CODE, Station_name, Station, Region)

# Food and EQS limits  
lookup_thresholds_1 <- read_excel("Input_data/Grenseverdier_fra_Sylvia.xlsx") %>%
  rename(EQS_threshold = EQS, Mattrygghet_threshold = Mattrygghet) %>%
  mutate(
    # Set EQS for PCB7
    EQS_threshold = case_when(
      PARAM == "CB_S7" ~ 0.6,
      TRUE ~ EQS_threshold),
    TISSUE_NAME = case_when(
      NIVA_CODE %in% "MU" ~ "Muskel",
      NIVA_CODE %in% "LI" ~ "Lever",
      NIVA_CODE %in% "SB" ~ "Whole soft body")
    )

# Check HARSAT values
# lookup_thresholds_harsat <- read.csv(
#   "https://raw.githubusercontent.com/osparcomm/HARSAT/develop/information/OSPAR_2022/thresholds_biota.csv")
# lookup_thresholds_harsat %>% filter(determinand == "TBSN+") %>% View("harsat")
# lookup_thresholds_harsat %>% filter(determinand == "VDS") %>% View("harsat")

lookup_thresholds_tbt <- data.frame(
  PARAM = "TBT",
  LATIN_NAME = c("Gadus morhua", "Littorina littorea", "Mytilus edulis", 
                 "N. lapillus / L. littorea", "Nucella lapillus"),
  TISSUE_NAME = c("Lever", "Whole soft body", "Whole soft body",
                  "Whole soft body", "Whole soft body"),
  EQS_threshold = 15.2, 
  Mattrygghet_threshold = 15.2
)

lookup_thresholds_vdsi <- data.frame(
  PARAM = "VDSI",
  LATIN_NAME = c("Littorina littorea", "N. lapillus / L. littorea", "Nucella lapillus"),
  TISSUE_NAME = c("Whole soft body", "Whole soft body", "Whole soft body"),
  EQS_threshold = 2, 
  Mattrygghet_threshold = NA
)

lookup_thresholds <- bind_rows(
  lookup_thresholds_1,
  lookup_thresholds_tbt,
  lookup_thresholds_vdsi
)


lookup_coordinates <- read.csv("Input_data/lookup_coordinates.csv")

# From Milkys2
# lookup_eqs <- read.csv("../Milkys2_pc/Files_from_Jupyterhub_2022/Lookup_tables/Lookup_EQS_limits.csv") %>%
#   filter(Basis %in% c("WW", "WWa")) %>%
#   select(-Long_name, -Kommentar) %>%
#   rename(EQS = Limit)

lookup_proref <- readRDS("Input_data/lookup_proref.rds")

# From Milkys2
# lookup_proref <- read.csv("../Milkys2_pc/Files_from_Jupyterhub_2022/Lookup_tables/Lookup_proref_ELU.csv") %>%
#   filter(Basis %in% c("WW", "WWa")) %>%
#   select(PARAM, LATIN_NAME, TISSUE_NAME, Basis, Proref) 

lookup_old_mussel <- read.csv2(
  "Data_export/2021 report North Sea blue mussel/MytiEdu_Snegl_2020_ver4.csv")
lookup_old_cod <- read.csv2(
  "Data_export/2021 report North Sea blue mussel/GaduMor_2020data_ver01.csv")

```

## Data

### NIVA data  

* Includes 20B   
* Includes sums (CB_S7, BDE6S)
* Lacking PROREF classification     
* Lacking EQS classification  
* Lacking HI cod data 


```{r}

dat_all <- readRDS("../Milkys2_pc/Files_from_Jupyterhub_2022/Raw_data/dat_raw5.rds") %>%
  left_join(lookup_stations %>% select(STATION_CODE, Station), by = "STATION_CODE") %>%
  rename(MYEAR = x) %>%
  mutate(
    VALUE_lo = case_when(
      is.na(Flag) ~ VALUE,
      !is.na(Flag) ~ 0),
  )

# xtabs(~transform, dat_all)
# xtabs(~PARAM + transform, dat_all)

dat_trend_all_01 <- readRDS("../Milkys2_pc/Files_from_Jupyterhub_2022/Trend_data/dat_trend.rds") %>%
    left_join(lookup_stations %>% select(STATION_CODE, Station), by = "STATION_CODE") %>%
    mutate(
      Perc_change = round((exp(-y_mean)-1)*100, 1),
      D_year = selected_year - First_year,
      Perc_annual = round((exp(-y_mean/D_year)-1)*100, 1),
      Perc_annual_lo = round((exp(-y_q2.5/D_year)-1)*100, 1),
      Perc_annual_hi = round((exp(-y_q97.5/D_year)-1)*100, 1)
    )

dat_trend_all_02 <- dat_trend_all_01 %>%
  mutate(
    Basis_selected = case_when(
      PARAM %in% "HG" ~ "WWa",
      TRUE ~ "WW")
  ) %>%
  filter(
    Trend_type == "short",
    Basis == Basis_selected) 

# table(subset(dat_trend_all_01, PARAM == "HG")$Basis)
# table(subset(dat_trend_all_02, PARAM == "HG")$Basis)
# table(dat_trend_all_02$Basis)

```


### Medians

* Using lower bound for assessment of levels

```{r}

dat_medians_1 <- dat_all %>%
  group_by(PARAM, STATION_CODE, LATIN_NAME, TISSUE_NAME, MYEAR) %>%
  summarise(
    Conc = median(VALUE_lo), .groups = "drop"
  )

dat_medians_2 <- dat_medians_1 %>%
  left_join(
    lookup_thresholds %>% select(PARAM, LATIN_NAME, TISSUE_NAMEME, EQS_threshold, Mattrygghet_threshold),
    by = c("PARAM", "LATIN_NAME", "TISSUE_NAME"))

dat_medians_3 <- dat_medians_2 %>%
  left_join(
    lookup_proref %>% select(PARAM, LATIN_NAME, PROREF),
    by = c("PARAM", "LATIN_NAME"))

dat_medians_4 <- dat_medians_3 %>%
  mutate(
    EQS = case_when(
      Conc/EQS_threshold <= 1 ~ 1,
      Conc/EQS_threshold > 1 ~ 2),
    Mattrygghet = case_when(
      Conc/Mattrygghet_threshold <= 1 ~ 1,
      Conc/Mattrygghet_threshold > 1 ~ 2),
  CLASS_proref = case_when(
    Conc/PROREF < 1 ~ 1,
    Conc/PROREF < 2 ~ 2,
    Conc/PROREF < 10 ~ 3,
    Conc/PROREF >= 10 ~4)
  )

```

### Add trends  

```{r}

dat_medians_5 <- dat_medians_4 %>%
  left_join(
    dat_trend_all_02 %>% select(PARAM, LATIN_NAME, TISSUE_NAME, STATION_CODE, Year_pick, Trend_string, Perc_annual),
    by = c("PARAM", "STATION_CODE", "LATIN_NAME", "TISSUE_NAME")
  )
 
```




## Blue mussel and snails  

### Select rows  

* We normally select the last year  
    - but if median concentration is under LOQ, the last year with data over LOQ is picked  
    - if no year has data over LOQ the last 10 years, no data  

```{r}

dat_medians_sel_1 <- dat_medians_5 %>%
  filter(
    STATION_CODE %in% unique(lookup_old_mussel$STATION_CODE),
    PARAM %in% unique(lookup_old_mussel$PARAM),
    MYEAR >= (selected_year - 10),
    !(PARAM != "VDSI" & Conc == 0)    
  ) %>%
  # Get last year with data over LOQ
  group_by(STATION_CODE, PARAM) %>%
  mutate(Last_year_over_LOQ = max(MYEAR)) %>%
  # Pick only that year
  filter(MYEAR == Last_year_over_LOQ) %>%
  select(-MYEAR) %>%
  # Add station name and coordinates
  left_join(
    lookup_coordinates %>% select(STATION_CODE, Station_short, Lat, Long),
    by = "STATION_CODE"
  )

```

### Add comparison with last year  

```{r}

df_old <- lookup_old_mussel %>%
  select(STATION_CODE, PARAM, trend, KLASSE, EQS, Mattrygghet)
names(df_old)[-(1:2)] <- paste0(names(df_old)[-(1:2)], "_old")
# df_old


dat_indicator <- dat_medians_sel_1 %>%
  left_join(
    df_old, by = c("PARAM", "STATION_CODE") 
  )

dat_indicator %>%
  select(STATION_CODE, PARAM, CLASS_proref, KLASSE_old)
# CD I301, I304
# HCB 56A fra 3 til 1

dat_indicator %>%
  select(STATION_CODE, PARAM, EQS, EQS_old)


```


### Saving  
```{r}

save_folder <- "Data_export/2022 report North Sea"
save_filename_csv <- "Indicators_North_sea_mussels_2022.csv"
save_filename_xlsx <- sub(".csv", "_fromR.xlsx", save_filename_csv, fixed = TRUE)

readr::write_csv(dat_indicator, paste0(save_folder, "/", save_filename_csv))
writexl::write_xlsx(dat_indicator, paste0(save_folder, "/", save_filename_xlsx))
  
# writexl::write_xlsx(dat_indicator, "Data_export/2022 report Barents Sea/Indicators_Barents_Sea_2022_ver7.xlsx")

```

## .
## ThE REST
## .

```{r}


ggplot(dat_extra_medians, aes(MYEAR, ymin = VALUE_lo, ymax = VALUE_up)) +
  geom_errorbar() +
  facet_grid(vars(PARAM), vars(STATION_CODE), scales = "free_y")


dat_extra_medians %>%
  group_by(PARAM, STATION_CODE, LATIN_NAME) %>%
  summarize(last_year = max(MYEAR), .groups = "drop") %>%
  pivot_wider(names_from = STATION_CODE, values_from = last_year) %>%
  kable() 
  
dat_extra_lastyear_1 <- dat_extra_medians %>%
  # data older than this are too old 
  filter(MYEAR >= 2016) %>%   
  # Arrange with last year first (so we can use first() in summarize)
  arrange(PARAM, STATION_CODE, desc(MYEAR)) %>%
  group_by(PARAM, STATION_CODE, LATIN_NAME) %>%
  summarize(
    Conc = first(VALUE_lo),
    MYEAR_last = first(MYEAR),  .groups = "drop")


```


### Time series     

```{r}

prev_results_mussel <- readxl::read_excel(
  "Data_export/2021 report North Sea blue mussel/MytiEdu_Snegl_2020_ver4.xlsx")

lookup_coordinates <- prev_results_mussel %>%
  group_by(PARAM, STATION_CODE) %>%
  summarise(
    LONGITUDE = first(LONGITUDE),
    LATITUDE = first(LATITUDE)
  )

```


### NIVA, medians  

```{r}

# code based on plot_timeseries2b() in '402_Plot_time_series_functions.R' (Milkys2) 

dat_niva_1 <- dat_all %>%
  ungroup() %>%
  filter(
    PARAM %in% prev_results_mussel$PARAM,
    STATION_CODE %in% prev_results_mussel$STATION_CODE) %>% # xtabs(~PARAM, .)
  mutate(y_comb = ifelse(is.na(y), threshold, y)) %>%
  group_by(PARAM, STATION_CODE, TISSUE_NAME, LATIN_NAME) %>%
  mutate(MYEAR_max = max(x)) %>%
  filter(x %in% MYEAR_max) %>% # xtabs(~PARAM, .)
  summarise(
    Conc = median(VALUE, na.rm = TRUE),
    n = n(),
    n_under_loq = sum(is.na(y)),
    .groups = "drop") #%>%
 #  mutate(overLOQ = n_overLOQ > (0.5*n))


```


### NIVA, trends  

```{r}

dat_niva_2 <- dat_niva_1 %>%
  left_join(
    dat_trend_all %>% filter(Trend_type %in% "short") %>% select(PARAM, STATION_CODE, Trend_string),
    by = c("PARAM", "STATION_CODE"), 
    relationship = "many-to-one")

```

### Complete data  

```{r}

dat <- dat_niva_2

```


### Add classes and thresholds  

```{r}

dat <- dat %>%
  left_join(
    lookup_thresholds %>% select(PARAM, LATIN_NAME, EQS_threshold, Mattrygghet_threshold),
    by = c("PARAM", "LATIN_NAME")) %>%
  left_join(
    lookup_proref %>% select(PARAM, LATIN_NAME, PROREF),
    by = c("PARAM", "LATIN_NAME")) %>%
  mutate(
    EQS = case_when(
      Conc/EQS_threshold <= 1 ~ 1,
      Conc/EQS_threshold > 1 ~ 2),
    Mattrygghet = case_when(
      Conc/Mattrygghet_threshold <= 1 ~ 1,
      Conc/Mattrygghet_threshold > 1 ~ 2),
  CLASS = case_when(
    Conc/PROREF < 1 ~ 1,
    Conc/PROREF < 2 ~ 2,
    Conc/PROREF < 10 ~ 3,
    Conc/PROREF >= 10 ~ 4)
  ) %>%
  left_join(
    lookup_coordinates
  )

```
### Check 

```{r}

xtabs(~CLASS + PARAM, dat)
xtabs(~KLASSE + PARAM, prev_results_mussel)

#ggplot(dat, aes(STATION_CODE, Conc, color = factor(CLASS, levels = 1:4))) +
ggplot(dat, aes(STATION_CODE, Conc, color = CLASS)) +
  geom_point() +
  scale_color_steps2() +
  facet_wrap("PARAM", scales = "free_y")

```



```{r}

# Add station names + Region 
  dat_all_prep3 <- dat_all_prep3 %>%
    left_join(lookup_stations %>% select(STATION_CODE, Station_name, Region), by = "STATION_CODE")
  
  # Add EQS and Proref to data    
  lookup_no_eqs <- dat_all_prep3 %>%
    distinct(PARAM) %>%
    anti_join(lookup_eqs, by = "PARAM")
  lookup_eqs_nospecies <- lookup_eqs %>%
    filter(is.na(LATIN_NAME))
  lookup_eqs_species <- lookup_eqs %>%
    filter(!is.na(LATIN_NAME))
  overlap <- intersect(lookup_eqs_species$PARAM, lookup_eqs_nospecies$PARAM)
  if (length(overlap) > 0)
    stop("In the EQS file, each parameter must have either LATIN_NAME empty, or LATIN_NAME for all rows")
  if (sum(is.na(lookup_eqs_species$Basis)) > 0)
    stop("In the EQS file, all rows with LATIN_NAME must also have Basis given")
  
  dat_all_prep3 <- bind_rows(
    dat_all_prep3 %>% 
      filter(PARAM %in% lookup_no_eqs$PARAM),
    dat_all_prep3 %>% 
      filter(PARAM %in% lookup_eqs_nospecies$PARAM) %>%
      left_join(lookup_eqs_nospecies %>% select(-LATIN_NAME, -Basis), by = c("PARAM")),
    dat_all_prep3 %>%
      filter(PARAM %in% lookup_eqs_species$PARAM) %>%
      left_join(lookup_eqs_species, by = c("PARAM", "LATIN_NAME", "Basis")),
  ) %>%
    left_join(lookup_proref, by = c("PARAM", "LATIN_NAME", "TISSUE_NAME", "Basis"))
  

```





### Read file    
```{r}

dat_indicator_draft1 <- readRDS("Data/14_df_indicator2_cod_ver01 (2022.rds")

params <- c("BDE6S", "CB_S7", "CD", "DDEPP", "HCB", "HG", "PB")
xtabs(~PARAM + STATION_CODE, dat_indicator_draft1 %>% filter(PARAM %in% params))  

# names(dat_indicator_draft1)

```

### Get trends from Milkys assessment 2021   

```{r}

milkys_big_table <- read_excel(
   "../Milkys2_pc/Files_from_Jupyterhub_2021/Big_excel_table/Data_xl_2022-09-23_ver09.xlsm", 
   col_types = "text"
)

df_trends_all <- milkys_big_table %>%
  select(PARAM, STATION_CODE, LATIN_NAME, TISSUE_NAME, Basis, Trends.2021) %>%
  filter(PARAM %in% params)


```

```{r}

trends_from_milkys_all <- readRDS(
  "../Milkys2_pc/Files_from_Jupyterhub_2021/Raw_data/126_df_trend_2021.rds")  
trends_from_milkys_all2 <- readRDS(
  "../Milkys2_pc/Files_from_Jupyterhub_2021/Raw_data/126_df_trend_wide_2021.rds")  

# trends_from_milkys_all <- readRDS(
#   "../Milkys2_pc/Files_from_Jupyterhub_2021/Big_excel_table/")  


trends_from_milkys1 <- trends_from_milkys_all %>%
  filter(
    STATION_CODE %in% c("10B", "19B", "20B", "43B2", "45B2", "98B1"),
    PARAM %in% params
  ) 
# xtabs(~STATION_CODE + x, trends_from_milkys1)  
# xtabs(~paste(STATION_CODE, PARAM) + x, trends_from_milkys1)  

trends_from_milkys <- trends_from_milkys1 %>%
  group_by(STATION_CODE, PARAM) %>%
  mutate(largest_x = max(x)) %>%
  filter(
    x == largest_x
  ) %>%
  ungroup()

xtabs(~PARAM + STATION_CODE, trends_from_milkys)  

```


### Plots   
```{r}

# Mattrygghet  
dat_indicator_draft1 %>% filter(PARAM %in% params) %>%
  ggplot(aes(PARAM, STATION_CODE, color = addNA(Mattrygghet))) +
  geom_point(size = rel(5))

# EQS  
dat_indicator_draft1 %>% filter(PARAM %in% params) %>%
  ggplot(aes(PARAM, STATION_CODE, color = addNA(EQS))) +
  geom_point(size = rel(5))

# Trends
# OK for NIVA data (lacking for 20B, that's normal)
# Lacking Barents trends! 
dat_indicator_draft1 %>% filter(PARAM %in% params) %>%
  ggplot(aes(PARAM, STATION_CODE, color = addNA(trend_text))) +
  geom_point(size = rel(5))

```

### Trends in Milkys assessment data  

```{r}



trends_from_milkys %>%
  ggplot(aes(PARAM, STATION_CODE, color = addNA(Trend_string))) +
  geom_point(size = rel(5))

trends_from_milkys %>%
  ungroup() %>%
  count(Trend_model, Trend_symbol)

trends_from_milkys %>%
  mutate(
    trend_text = case_when(
      Trend_string %in% "Decreasing" ~ "Trend_down",
      Trend_string %in% "Increasing" ~ "Trend_up",
      Trend_string %in% "No change" ~ "No trend"
    )
  ) %>%
  ggplot(aes(PARAM, STATION_CODE, color = addNA(trend_text))) +
  geom_point(size = rel(5)) +
  geom_text(aes(label = x), color = "black")

```



## Add EQS classification  
```{r}

dat_indicator_draft1 <- dat_indicator_draft1 %>%
  rename(EQS_threshold = EQS) %>%
  mutate(
    EQS = case_when(
      Conc >= EQS_threshold ~ 2,
      Conc < EQS_threshold ~ 1)
  )

```

## Function for getting slope  

```{r}

# Make function that returns a data frame with lower and upper limits of the slope
#   for a given function and for a given data set
# (Can modify this so it also works for a specific parameter, but in this case we usi)

get_trendslope <- function(parameter, stationcode, data){
  dat_selected <- data %>%
    filter(PARAM %in% parameter & STATION_CODE %in% stationcode)
  dat_interval <- dat_selected %>%
    filter(VALUE_lo < VALUE_up)
  if (nrow(dat_interval) >= 1){
    # If there are actual intervals, run int_linear_qi
    mod <- int_linear_qi(data = dat_selected, x = "MYEAR", y_lo = "VALUE_lo", y_up = "VALUE_up")
    result <- data.frame(
      PARAM = parameter,
      STATION_CODE = stationcode,
      slope_lo = mod$slope[["2.5%"]],
      slope = mod$slope[["50%"]],
      slope_up = mod$slope[["97.5%"]])
  } else {
    # If there are no intervals (alll data over LOQ), run lm
    mod <- lm(VALUE_up ~ MYEAR, data = dat_selected)  
    summ <- summary(mod)
    result <- data.frame(
      PARAM = parameter,
      STATION_CODE = stationcode,
      slope_lo =  summ$coefficients[2,1] - 2*summ$coefficients[2,2],
      slope =  summ$coefficients[2,1],
      slope_up = summ$coefficients[2,1] + 2*summ$coefficients[2,2])
  }
  result
}

if (FALSE){
  
  # *** test *** 
  
  # debugonce(get_trendslope)
  test_trendslope1 <- get_trendslope("HG", "10A2", data = dat_bluemussel_medians)  # 10A2 11X 98A2
  test_trendslope2 <- get_trendslope("CB_S7", "10A2", data = dat_bluemussel_medians) 
  test_trendslope3 <- get_trendslope("HG", "HI_Barents1", data = dat_nifes_median)
  test_trendslope4 <- get_trendslope("BDE6S", "HI_Barents2", data = dat_nifes_median)
  
  # test_trendslope1
  
}

# Make "safe" version (doen't stop if there is an error)
get_trendslope_s <- safely(get_trendslope)


```


## Prepare extra NIVA data  

* blue mussel  
* HCB and DDEPP (to 2020 only)  

### Get blue mussel data  

```{r}

stations_sel <- c("10A2", "11X", "98A2")
params_ind <- c("CD", "DDEPP", "HCB", "HG", "PB")
params_pcb <- c("CB28", "CB52", "CB101", "CB118", "CB138", "CB153", "CB180")
params_bde <- c("BDE28", "BDE47", "BDE99", "BDE100", "BDE153", "BDE154")

# Latest NIVA data ----
dat_all <- readRDS("Input_data/NIVA_data_2022-09-01.rds")

# 
# Select stations and compunds
#
dat_bluemussel_all <- dat_all %>%
  filter(
    # STATION_CODE %in% c("10A2", "10B", "11X", "19B", "43B2", "45B2", "98A2", "98B1", "20B"),
    STATION_CODE %in% stations_sel,
    PARAM %in% c(params_ind, params_pcb, params_bde)
  ) %>%
  mutate(
    VALUE_lo = ifelse(is.na(FLAG1), VALUE_WW, 0),
    VALUE_up = VALUE_WW
  ) %>%
  select(STATION_CODE, SAMPLE_NO2, MYEAR, PARAM, VALUE_WW, FLAG1, VALUE_lo, VALUE_up)

xtabs(~PARAM + STATION_CODE, dat_bluemussel_all)  

```


### Get raw data for blue mussel      

```{r}

#
# Individual compounds
# 
dat_bluemussel_ind <- dat_bluemussel_all %>%
  filter(PARAM %in% params_ind)

#
# PCB
#
dat_bluemussel_pcb <- dat_bluemussel_all %>%
  filter(PARAM %in% params_pcb) %>%
  group_by(STATION_CODE, SAMPLE_NO2, MYEAR) %>%
  summarise(
    VALUE_lo = sum(VALUE_lo),
    VALUE_up = sum(VALUE_up), .groups = "drop"
  ) %>%
  mutate(
    PARAM = "CB_S7")

#
# BDE
#
dat_bluemussel_bde <- dat_bluemussel_all %>%
  filter(PARAM %in% params_bde) %>%
  group_by(STATION_CODE, SAMPLE_NO2, MYEAR) %>%
  summarise(
    VALUE_lo = sum(VALUE_lo),
    VALUE_up = sum(VALUE_up), .groups = "drop"
  ) %>%
  mutate(
    PARAM = "BDES6")

dat_bluemussel_raw <- bind_rows(
  dat_bluemussel_ind,
  dat_bluemussel_pcb,
  dat_bluemussel_bde)  %>%
  # Next 3 lines: keep only time series with data after 2016 
  group_by(STATION_CODE, PARAM) %>%
  mutate(Last_year = max(MYEAR)) %>%
  filter(Last_year >= 2017) %>%
  ungroup()


# xtabs(~PARAM + STATION_CODE, dat_bluemussel_all)  
# xtabs(~MYEAR + STATION_CODE, dat_bluemussel_bde)  
xtabs(~PARAM + STATION_CODE, dat_bluemussel_raw)  

```


### Get data for cod HCB and DDE(PP)     

```{r}

# dat_all2 <- readRDS("../Milkys2_pc/Files_from_Jupyterhub_2021/Raw_data/109_adjusted_data_2022-09-23.rds")

stations_sel <- c("10B", "19B", "98B1", "20B", "43B2", "45B2")
params_ind <- c("DDEPP", "HCB")

# 
# Select stations and compunds
#
dat_cod_pesticides_raw <- dat_all %>%
  filter(
    # STATION_CODE %in% c("10A2", "10B", "11X", "19B", "43B2", "45B2", "98A2", "98B1", "20B"),
    STATION_CODE %in% stations_sel,
    LATIN_NAME == "Gadus morhua",
    PARAM %in% c(params_ind)
  ) %>%
  # Next 3 lines: keep only time series with data after 2016 
  group_by(STATION_CODE, PARAM) %>%
  mutate(Last_year = max(MYEAR)) %>%
  filter(Last_year >= 2017) %>%
  mutate(
    VALUE_lo = ifelse(is.na(FLAG1), VALUE_WW, 0),
    VALUE_up = VALUE_WW
  ) %>%
  select(STATION_CODE, SAMPLE_NO2, MYEAR, PARAM, VALUE_WW, FLAG1, VALUE_lo, VALUE_up)

xtabs(~PARAM + STATION_CODE, dat_cod_pesticides_raw)  
# xtabs(~PARAM + MYEAR, dat_cod_pesticides_raw)  

```

### Combine blue mussel and cod  
```{r}

dat_extra_raw <- bind_rows(
  dat_bluemussel_raw %>% mutate(LATIN_NAME = "Mytilus edulis"),
  dat_cod_pesticides_raw %>% mutate(LATIN_NAME = "Gadus morhua")
)

```


### Get medians  
```{r}

dat_extra_medians <- dat_extra_raw %>%
  group_by(PARAM, STATION_CODE, LATIN_NAME, MYEAR) %>%
  summarise(
    VALUE_lo = median(VALUE_lo),
    VALUE_up = median(VALUE_up), .groups = "drop"
  )

ggplot(dat_extra_medians, aes(MYEAR, ymin = VALUE_lo, ymax = VALUE_up)) +
  geom_errorbar() +
  facet_grid(vars(PARAM), vars(STATION_CODE), scales = "free_y")

```

### Get last year's value  

* Called Conc  

```{r}

dat_extra_medians %>%
  group_by(PARAM, STATION_CODE, LATIN_NAME) %>%
  summarize(last_year = max(MYEAR), .groups = "drop") %>%
  pivot_wider(names_from = STATION_CODE, values_from = last_year) %>%
  kable() 
  
dat_extra_lastyear_1 <- dat_extra_medians %>%
  # data older than this are too old 
  filter(MYEAR >= 2016) %>%   
  # Arrange with last year first (so we can use first() in summarize)
  arrange(PARAM, STATION_CODE, desc(MYEAR)) %>%
  group_by(PARAM, STATION_CODE, LATIN_NAME) %>%
  summarize(
    Conc = first(VALUE_lo),
    MYEAR_last = first(MYEAR),  .groups = "drop")

```


### Lookup tables  

* Thresholds  
* Proref  
* Coordinates

```{r}

# Food and EQS limits  
lookup_thresholds <- read_excel("Input_data/Grenseverdier_fra_Sylvia.xlsx") %>%
  rename(EQS_threshold = EQS, Mattrygghet_threshold = Mattrygghet)

# Set EQS for PCB7
lookup_thresholds <- lookup_thresholds %>%
  mutate(
    EQS_threshold = case_when(
      PARAM == "CB_S7" ~ 0.6,
      TRUE ~ EQS_threshold))


# lookup_thresholds_bluemussel <- lookup_thresholds %>%
#   rename(EQS_threshold = EQS, Mattrygghet_threshold = Mattrygghet)

# Proref (
# lookup_proref <- readRDS("Input_data/lookup_proref.rds")

lookup_proref <- readRDS("Input_data/lookup_proref.rds")


# lookup_proref_bluemussel <- lookup_proref %>%
#   filter(LATIN_NAME == "Mytilus edulis")

# Coordinates  
lookup_coord <- readRDS("Input_data/lookup_coordinates.rds")


```


### Classify regarding limits  
```{r}

n1 <- nrow(dat_extra_lastyear_1)

dat_extra_lastyear_2 <- dat_extra_lastyear_1 %>%
  left_join(
    lookup_thresholds %>% select(PARAM, LATIN_NAME, EQS_threshold, Mattrygghet_threshold),
    by = c("PARAM", "LATIN_NAME")) %>%
  left_join(
    lookup_proref %>% select(PARAM, LATIN_NAME, PROREF),
    by = c("PARAM", "LATIN_NAME")) %>%
  mutate(
    EQS = case_when(
      Conc/EQS_threshold <= 1 ~ 1,
      Conc/EQS_threshold > 1 ~ 2),
    Mattrygghet = case_when(
      Conc/Mattrygghet_threshold <= 1 ~ 1,
      Conc/Mattrygghet_threshold > 1 ~ 2),
  CLASS = case_when(
    Conc/PROREF < 1 ~ 1,
    Conc/PROREF < 2 ~ 2,
    Conc/PROREF < 10 ~ 3,
    Conc/PROREF >= 10 ~ 4)
  )
 
n2 <- nrow(dat_extra_lastyear_2)  

if (n2 > n1)
  stop("Number of rows increased, check left joins!")


# Check EQS
# dat_extra_lastyear_2 %>%
#   count(PARAM, EQS_threshold, EQS) %>%
#   kable()


```

### Get trends    

```{r}

# data must contain PARAM, STATION_CODE, VALUE_lo, VALUE_up
# relies on get_trendslope_s
get_trendslope_all <- function(data){
  
  # Get all combinations:
  df_combinations <- data %>%
    distinct(PARAM, STATION_CODE) %>%
    as.data.frame()
  
  df_slopes_list <- map2(df_combinations$PARAM, df_combinations$STATION_CODE, get_trendslope_s, 
                                data = data)
  
  df_slopes_list <- transpose(df_slopes_list)  
  
  ok <- map_lgl(df_slopes_list$error, is.null)  
  # ok
  cat("\n=============================================================\n")
  cat(round(100*mean(ok), 0), " percent of analyses succeeded (", sum(ok), " of a total of ", length(ok), ")\n", sep = "")
  cat("=============================================================\n")
  
  df_slopes <- bind_rows(df_slopes_list$result[ok])
  
  df_slopes
  
}


if (FALSE){
  
  dat_extra_trends <- get_trendslope_all(dat_extra_medians)
  
  # debugonce(get_trendslope_all)
  # dat_extra_trends_test <- get_trendslope_all(dat_extra_medians)
  
  # 95 percent of analyses succeeded (20 of a total of 21)
  
  saveRDS(dat_extra_trends, "Data/15_dat_extra_trends.rds")
  write.csv(dat_extra_trends, "Data/15_dat_extra_trends.csv")
  
}

dat_extra_trends <- readRDS("Data/15_dat_extra_trends.rds")

```


### Add trend columns   

* Creates 'dat_indicator_extra'  

```{r}

n1 <- nrow(dat_extra_lastyear_2)

dat_indicator_extra <- dat_extra_lastyear_2 %>%
  left_join(dat_extra_trends, by = c("STATION_CODE", "PARAM")) %>%
  left_join(lookup_coord %>% select(STATION_CODE, Long, Lat), by = "STATION_CODE") %>%
  rename(LATITUDE = Lat, LONGITUDE = Long) %>%
  select(-Mattrygghet_threshold, -EQS_threshold)

n2 <- nrow(dat_indicator_extra)

if (n2 > n1)
  stop("Number of rows increased, check left joins!")

```

### Add rows to dat_indicator   

```{r}

check <- dat_indicator_draft1 %>%
  inner_join(dat_indicator_extra %>% select(STATION_CODE, PARAM))

if (nrow(check) > 0)
  stop("Some STATION_CODE x PARAM combinations already exist in dat_indicator_draft1")

dat_indicator_draft2 <- dat_indicator_draft1 %>%
  bind_rows(dat_indicator_extra %>% select(-MYEAR_last))

n1 <- names(dat_indicator_draft1)
n2 <- names(dat_indicator_extra)

setdiff(n1, n2)
setdiff(n2, n1)

```


## Trends for HI (aka NIFES) cod   

* Based on script 91

### Data   

```{r}

#
# Raw data for HI (NIFES) cod    
# - includes CB_S7 and BDE6S
#
dat_hi <- readRDS("Data/11_df_nifes_cod (2022).rds")
xtabs(~PARAM + STATION_CODE, dat_hi)

# names(dat_hi)

```


### Data for individual parameters    

```{r}

# params

# xtabs(~Prøvenr. + STATION_CODE, dat_hi %>% filter(Year == 2022))
# xtabs(~Jnr + STATION_CODE, dat_hi %>% filter(Year == 2022))
# dat_hi %>% filter(Prøvenr. == "2022-1450") %>% View()

dat_nifes_raw1 <- dat_hi %>%
  filter(
    STATION_CODE %in% c("HI_Barents1", "HI_Barents2", "HI_Barents3"),
    PARAM %in% c("CD", "DDEPP", "HCB", "HG", "PB")  
  ) %>%
  mutate(
    Ind = paste0(Jnr, "__", Prøvenr.),
    VALUE_lo = ifelse(grepl("<", Flag), 0, Conc),
    VALUE_up = Conc) %>%
  rename(
    MYEAR = Year) %>%
  select(STATION_CODE, Ind, MYEAR, PARAM, Conc, Flag, VALUE_lo, VALUE_up)


```


### Data for sum parameters    

```{r}
#
# Calculate upper/lower bounds of PCB7 ----
#

#
# PCB
#
dat_nifes_raw2_before_sum <- dat_hi %>%
  filter(
    STATION_CODE %in% c("HI_Barents1", "HI_Barents2", "HI_Barents3"),
    PARAM %in% c("CB28", "CB52", "CB101", "CB118", "CB138", "CB153", "CB180")  # for BDE, replace here 
  ) %>%
  mutate(
    Ind = paste0(Jnr, "__", Prøvenr.),
    VALUE_lo = ifelse(grepl("<", Flag), 0, Conc),
    VALUE_up = Conc) %>%
  rename(
    MYEAR = Year) %>%
  select(STATION_CODE, Ind, MYEAR, PARAM, Conc, Flag, VALUE_lo, VALUE_up)
  
dat_nifes_raw2 <- dat_nifes_raw2_before_sum %>%
  group_by(STATION_CODE, Ind, MYEAR) %>%
  summarise(
    VALUE_lo = sum(VALUE_lo),
    VALUE_up = sum(VALUE_up), .groups = "drop"
  ) %>%
  mutate(
    PARAM = "CB_S7")


#
# BDE
#
dat_nifes_raw3_before_sum <- dat_hi %>%
  filter(
    STATION_CODE %in% c("HI_Barents1", "HI_Barents2", "HI_Barents3"),
    PARAM %in% c("BDE 28", "BDE 47", "BDE 99", "BDE 100", "BDE 153", "BDE 154")
  ) %>%
  mutate(
    Ind = paste0(Jnr, "__", Prøvenr.),
    VALUE_lo = ifelse(grepl("<", Flag), 0, Conc),
    VALUE_up = Conc) %>%
  rename(
    MYEAR = Year) %>%
  select(STATION_CODE, Ind, MYEAR, PARAM, Conc, Flag, VALUE_lo, VALUE_up)
  
dat_nifes_raw3 <- dat_nifes_raw3_before_sum %>%
  group_by(STATION_CODE, Ind, MYEAR) %>%
  summarise(
    VALUE_lo = sum(VALUE_lo),
    VALUE_up = sum(VALUE_up), .groups = "drop"
  ) %>%
  mutate(
    PARAM = "BDE6S")

```


#### Check BDE sum  

```{r}

ggplot(dat_nifes_raw3, aes(MYEAR, VALUE_up)) +
  geom_jitter(width = 0.2) +
  geom_jitter(aes(y = VALUE_lo), shape = 1, color = "red", width = 0.2) 
  
  
```

### Data, combined   

```{r}

dat_nifes_raw <- bind_rows(
  dat_nifes_raw1, dat_nifes_raw2, dat_nifes_raw3
)

dat_nifes_median <- dat_nifes_raw %>%
  group_by(PARAM, STATION_CODE, MYEAR) %>%
  summarise(
    VALUE_lo = median(VALUE_lo),
    VALUE_up = median(VALUE_up)
  )

xtabs(~PARAM, dat_nifes_raw)
xtabs(~PARAM, dat_nifes_median)

```

### Plots 

```{r}

ggplot(dat_nifes_median, aes(MYEAR, ymin = VALUE_lo, ymax = VALUE_up)) +
  geom_errorbar() +
  facet_wrap(vars(PARAM), scales = "free_y")

ggplot(dat_nifes_median, aes(MYEAR)) +
  geom_point(aes(y = VALUE_up), color = "red") +
  geom_point(aes(y = VALUE_lo), color = "blue") +
  facet_grid(vars(PARAM), vars(STATION_CODE), scales = "free_y")

```

### Fix BDE sum in 'dat_indicator_draft2'  

- Creating dat_indicator_draft2b

```{r}

dat_indicator_draft2 %>%
  filter(grepl("HI_Barents", STATION_CODE) & PARAM == "BDE6S")  

dat_nifes_median_bde6S <- dat_nifes_median %>%
  filter(grepl("HI_Barents", STATION_CODE) & PARAM == "BDE6S") %>%
  group_by(STATION_CODE, PARAM) %>%
  mutate(MYEAR_max = max(MYEAR)) %>%
  filter(MYEAR == MYEAR_max) %>%
  select(STATION_CODE, PARAM, VALUE_lo)

dat_indicator_draft2b <- dat_indicator_draft2 %>%
  left_join(dat_nifes_median_bde6S, by = join_by(STATION_CODE, PARAM)) %>%
  mutate(
    Conc = case_when(
      !is.na(VALUE_lo) ~ VALUE_lo,
      is.na(VALUE_lo) ~ Conc),
    CLASS = case_when(
      is.na(VALUE_lo) ~ CLASS,
      Conc/PROREF < 1 ~ 1,
      Conc/PROREF < 2 ~ 2,
      Conc/PROREF < 10 ~ 3,
      Conc/PROREF >= 10 ~ 4)
  )

```


### Get all slopes  

```{r}

# Make "safe" version (doen't stop if there is an error)
get_trendslope_s <- safely(get_trendslope)

# Get all combinations:
df_combinations <- dat_nifes_median %>%
  distinct(PARAM, STATION_CODE) %>%
  as.data.frame()

# df_combinations <- df_combinations %>%
#   filter(PARAM == "BDE6S")

if (FALSE){
  
  dat_nifes_trends_list <- map2(df_combinations$PARAM, df_combinations$STATION_CODE, get_trendslope_s, 
                                data = dat_nifes_median)
  
  dat_nifes_trends_list <- transpose(dat_nifes_trends_list)  
  
  ok <- map_lgl(dat_nifes_trends_list$error, is.null)  
  # ok
  
  dat_nifes_trends <- bind_rows(dat_nifes_trends_list$result[ok])
  
  # file.copy("Data/15_dat_nifes_trends.rds", "Data/15_dat_nifes_trends_OLD.rds")   # copy 23-04-2024
  saveRDS(dat_nifes_trends, "Data/15_dat_nifes_trends.rds")
  write.csv(dat_nifes_trends, "Data/15_dat_nifes_trends.csv")
  
  # dat_nifes_trends %>% filter(PARAM == "BDE6S") %>% readr::write_csv("clipboard")
  
}

dat_nifes_trends <- readRDS("Data/15_dat_nifes_trends.rds")

```


```{r}

if (FALSE){
  
  # Compare old and new trend
  
  dat_nifes_trends_old <- readRDS("Data/15_dat_nifes_trends_OLD.rds")
  
  check <- bind_rows(
    dat_nifes_trends %>% mutate(run = "new"),
    dat_nifes_trends_old %>% mutate(run = "old")
  ) 
  
  ggplot(check, aes(paste(STATION_CODE, run), slope, color = run)) +
    geom_pointrange(aes(ymin = slope_lo, ymax = slope_up)) +
    facet_wrap(vars(PARAM), scale = "free_y")
  
}

```



## Add slopes for HI data   

* Adding slopes to indicator data (dat_indicator_draft2b)
* Creates dat_indicator

### Combine and check trend  
```{r}

head(dat_indicator_draft2b)

dat_indicator <- bind_rows(
  # Rows with HI stations: remove slope columns and ad new ones from 'dat_nifes_trends'  
  dat_indicator_draft2b %>% 
    filter(STATION_CODE %in% c('HI_Barents1', 'HI_Barents2', 'HI_Barents3')) %>%
    mutate(
      PARAM = case_when(
        PARAM %in% "PP-DDE" ~ "DDEPP",
      PARAM %in% "HEXACHLOROBENZENE (HCB)" ~ "HCB",
      TRUE ~ PARAM)) %>%
    select(-slope_lo, -slope, -slope_up) %>%
    left_join(dat_nifes_trends, by = c("PARAM", "STATION_CODE")),
  # Rows without HI stations: keep as they are    
  dat_indicator_draft2b %>% 
    filter(!STATION_CODE %in% c('HI_Barents1', 'HI_Barents2', 'HI_Barents3')) 
)

dat_indicator %>%
  count(trend_text)

dat_indicator <- dat_indicator %>%
  filter(!PARAM %in% c("HEXACHLOROBENZENE (HCB)", "OP-DDD", "OP-DDE", "OP-DDT", "PBDE 100", 
                       "PBDE 119", "PBDE 138", "PBDE 153", "PBDE 154", "PBDE 183", "PBDE 28", 
                       "PBDE 47", "PBDE 66", "PBDE 99", "PCB-101", "PCB-118", "PCB-138", 
                       "PCB-153", "PCB-180", "PCB-28", "PCB-52", "PP-DDD", "PP-DDE", 
                       "PP-DDT", "SUM PCB 6")) %>%
  mutate(
    trend_text2 = case_when(
    slope_lo > 0 ~ "Trend_up",
    slope_up < 0 ~ "Trend_down",
    !is.na(slope_lo) ~ "No trend")
  )
  
dat_indicator %>%
  count(trend_text, trend_text2)  


```

### Replace 'trend_text' by 'trend_text2' values   
```{r}

dat_indicator <- dat_indicator %>%
  mutate(trend_text = trend_text2) %>%
  select(-trend_text2)

```


## Fixing stuff

### Fixing trend code and trend text    

```{r}

# xtabs(~addNA(trend) + addNA(trend_text), dat_indicator)

dat_indicator <- dat_indicator %>%
  mutate(
    trend = case_when(
      !is.na(trend) ~ trend,
      is.na(trend_text) ~ 0,
      trend_text == "No trend" ~ 1,
      trend_text == "Trend_up" ~ 2,
      trend_text == "Trend_down" ~ 3),
    trend_text = case_when(
      !is.na(trend_text) ~ trend_text,
      is.na(trend_text) ~ "Not calc.")
  )

xtabs(~addNA(trend) + addNA(trend_text), dat_indicator)

```


### Fixing station name      

```{r}

# xtabs(~addNA(trend) + addNA(trend_text), dat_indicator)

df_stations <- readRDS("Input_data/NIVA_data_stations.rds")

dat_indicator <- dat_indicator %>%
  left_join(df_stations %>% select(STATION_CODE, STATION_NAME2 = STATION_NAME)) %>%
  mutate(
    STATION_NAME = case_when(
      !is.na(STATION_NAME) ~ STATION_NAME,
      STATION_CODE == "HI_Barents1" ~ "Barentshavet 1 (HI)",
      STATION_CODE == "HI_Barents2" ~ "Barentshavet 2 (HI)",
      STATION_CODE == "HI_Barents3" ~ "Barentshavet 3 (HI)",
      TRUE ~ STATION_NAME2)
  ) %>%
  select(-STATION_NAME2)


dat_indicator %>%
  count(STATION_CODE, STATION_NAME) %>%
  kable()

```


### Classify regarding limits (again!)   


```{r}

# dat_indicator_back <- dat_indicator
# restore:
# dat_indicator <- dat_indicator_back

# Checking for duplicates of PARAM x LATIN_NAME
check <- lookup_thresholds %>%
  add_count(PARAM, LATIN_NAME) %>%
  filter(n > 1)

lookup_thresholds <- lookup_thresholds %>%
  filter(!(PARAM == "CB_S7" & NIVA_CODE == "MU"))

# Fix error on BDE name
sel <- dat_indicator$PARAM %in% "BDES6"
dat_indicator$PARAM[sel] <- "BDE6S"

n1 <- nrow(dat_indicator)

dat_indicator <- dat_indicator %>%
  select(-EQS_threshold, -PROREF) %>%
  left_join(
    lookup_thresholds %>% select(PARAM, LATIN_NAME, EQS_threshold, Mattrygghet_threshold),
    by = c("PARAM", "LATIN_NAME")) %>%
  left_join(
    lookup_proref %>% select(PARAM, LATIN_NAME, PROREF),
    by = c("PARAM", "LATIN_NAME")) %>%
  mutate(
    EQS = case_when(
      Conc/EQS_threshold <= 1 ~ 1,
      Conc/EQS_threshold > 1 ~ 2),
    Mattrygghet = case_when(
      Conc/Mattrygghet_threshold <= 1 ~ 1,
      Conc/Mattrygghet_threshold > 1 ~ 2),
    CLASS = case_when(
      Conc/PROREF < 1 ~ 1,
      Conc/PROREF < 2 ~ 2,
      Conc/PROREF < 10 ~ 3,
      Conc/PROREF >= 10 ~ 4)
  )


n2 <- nrow(dat_indicator)  

if (n2 > n1)
  stop("Number of rows increased, check left joins!")


# Check EQS
# dat_indicator %>%
#   count(PARAM, EQS_threshold, EQS) %>%
#   kable()


```

### Check EQS        

```{r}

dat_indicator %>%
  count(PARAM, EQS_threshold, EQS) %>%
  kable()

```


## Plots for dat_indicator  
```{r}

# trends

# Mattrygghet  
dat_indicator %>% filter(PARAM %in% params) %>%
  ggplot(aes(PARAM, STATION_CODE, color = addNA(Mattrygghet))) +
  geom_point(size = rel(5))

# EQS  
dat_indicator %>% filter(PARAM %in% params) %>%
  ggplot(aes(PARAM, STATION_CODE, color = addNA(EQS))) +
  geom_point(size = rel(5))

# PROREF class 
dat_indicator %>% filter(PARAM %in% params) %>%
  ggplot(aes(PARAM, STATION_CODE, color = addNA(CLASS))) +
  geom_point(size = rel(5)) +
  scale_color_brewer() +
  theme_dark()

# Trends
# OK for NIVA data (lacking for 20B, that's normal)
# Lacking Barents trends! 
dat_indicator %>% filter(PARAM %in% params) %>%
  ggplot(aes(PARAM, STATION_CODE, color = addNA(trend_text))) +
  geom_point(size = rel(5))

```

## Add positions for HI stations    

### Check  
```{r}

# dat_indicator %>%
#   count(STATION_CODE, LATITUDE, LONGITUDE)  

dat_positions_hi <- dat_hi %>%
  distinct(STATION_CODE, LATITUDE, LONGITUDE)  

dat_positions_hi

```

### Add coordinates for HI stations   
```{r}


dat_indicator <- bind_rows(
  # Rows with HI stations: remove slope columns and ad new ones from 'dat_indicator'  
  dat_indicator %>% 
    filter(STATION_CODE %in% c('HI_Barents1', 'HI_Barents2', 'HI_Barents3')) %>% 
    select(-LATITUDE, -LONGITUDE) %>%
    left_join(dat_positions_hi),
  # Rows without HI stations: keep as they are    
  dat_indicator %>% 
    filter(!STATION_CODE %in% c('HI_Barents1', 'HI_Barents2', 'HI_Barents3'))
)

```

### Check again   
```{r}

dat_indicator %>%
  count(STATION_CODE, LATITUDE, LONGITUDE)  


```
## Save  

### Add units  
```{r}

dat_indicator <- dat_indicator %>%
  mutate(
    Conc_unit = case_when(
      PARAM %in% c("BDE6S", "CB_S7", "HCB", "DDEPP") ~ "ug/kg w.w.",
      PARAM %in% c("CD", "HG", "PB") ~ "mg/kg w.w."),
    .after = Conc) %>%
  mutate(
    EQS_unit = Conc_unit, .after = EQS_threshold) %>%
  mutate(
    PROREF_unit = Conc_unit, .after = PROREF)


```

```{r}

unique(dat_indicator$PARAM)

```


### Saving  
```{r}

saveRDS(dat_indicator, "Data_export/2022 report Barents Sea/Indicators_Barents_Sea_2022.rds")
write.csv(dat_indicator, "Data_export/2022 report Barents Sea/Indicators_Barents_Sea_2022.csv")

writexl::write_xlsx(dat_indicator, "Data_export/2022 report Barents Sea/Indicators_Barents_Sea_2022_ver7.xlsx")

```




## APPENDIX

### Get and save PROREF values  
```{r}

if (FALSE){
  
  # 
  # Get and save PROREF values
  #
  
  coltypes = c(rep("text",18),                  # PARAM - "Reference stations" [column A - R]
               rep("numeric", 4),               # "Reference station count" - "PROREF" [column S - V]
               rep(c("numeric","text"),41),     # Data 1980 - 2019 (2 columns per year) - INCREASE THIS NUMBER BY 1 EACH YEAR
               "text","numeric","numeric","numeric","numeric",  # "N_string" - "EQS-threshold" last year 
               "text","numeric","numeric","numeric","numeric",  # "N_string" - "EQS-threshold" this year 
               "text",                                          # Dummy [DI]
               rep("numeric", 10),                              # "Trend p(long) this year" - "No. of Years(short) this year" 
               "text"                           # "Trends this year" [DV]
  )
  
  #
  # FOR 2020
  #
  
  fn <- "../Milkys2_pc/Files_from_Jupyterhub_2020/Big_excel_table/Data_xl_2021-09-15_ver03.xlsm"
  
  # For getting number of rows:
  data_xl_firstcol <- read_excel(fn, range = cell_cols("A:B"))

  # 
  # HARD-CODED for 2020! last column of 'data_xl_orig' - increases by 2 for each year
  #
  last_column <- "DV"
  range_2020 <- paste0("A1:", last_column, nrow(data_xl_firstcol))
  
  data_xl <- read_excel(fn,
                      range = range_2020,   
                      col_types = coltypes)
  
  # For demo:
  # data_xl %>% 
  #   filter(`Parameter Code` == "HG" & Basis == "WW" & `Station Code` == "43B2") %>%
  #   select(`Parameter Code`, Basis, `Station Code`, `Station Name`, Species, PROREF, V19, V20)
  
  lookup_proref <- data_xl %>% 
    filter(Basis %in% "WW" & !is.na(PROREF)) %>%
    rename(
      PARAM = `Parameter Code`,
      LATIN_NAME = Species) %>%
    distinct(PARAM, LATIN_NAME, PROREF)

  check <- lookup_proref %>% add_count(PARAM, LATIN_NAME) %>% filter(n > 1)
  if (nrow(check) > 0)
    stop("More than one PROREF per parameter * species!")
      
  # Save
  saveRDS(lookup_proref, "Input_data/lookup_proref.rds")
  
  rm(data_xl)
  
  
}

```


### Get and save coordinates  
```{r}

if (FALSE){
  
  data_coord <- read_excel("../Milkys2_pc/Files_to_Jupyterhub_2021/Kartbase_edit.xlsx")
  
  saveRDS(data_coord, "Input_data/lookup_coordinates.rds")
  write.csv(data_coord, "Input_data/lookup_coordinates.csv")

}


```

